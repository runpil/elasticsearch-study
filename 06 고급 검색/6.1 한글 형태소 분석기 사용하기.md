# 한글 형태소 분석기 사용하기
- 엘라스틱서치에서 한글 문서를 효율적으로 검색하게 하려면 한글 형태소 분석기를 활용해 직접 분석기를 구성해야 한다.
- 한글은 다른 언어와 달라 조사나 어미의 접미사가 명사, 동사 등과 결합하기 때문에 형태소를 분석하는 과정이 쉽지 않다.

## 은전한닢 형태소 분석기
- 은전한닢은 Mecab-co-dic 기반으로 만들어진 한국어 형태소 분석기
- 시스템 사전에 등록돼 있는 사전을 기반으로 동작하며, 복합명사와 활용어의 원형 찾기가 가능하다.

### 설치 방법
```
./bin/elasticsearch-plugin install https://github.com/javacafe-project/elastic-book-etc/raw/master/plugin/elasticsearch-analysis-seunjeon-6.4.3.zip
```

### 샘플 인덱스 생성
```
PUT /seunjeon_default_analyzer
{
    "settings":{
        "number_of_shards": 5,
        "number_of_replicas": 1,
        "index": {
            "analysis": {
                "analyzer": {
                    "korean": {
                        "type": "custom",
                        "tokenizer": "seunjeon_default_tokenizer"
                    }
                },
                "tokenizer": {
                    "seunjeon_default_tokenizer": {
                        "type": "seunjeon_tokenizer",
                        "index_eojeol": false,
                        "user_words": [
                            "낄끼+빠빠,-100", "c\\+\\+", "어그로", "버카총", "abc 마트"
                        ]
                    }
                }
            }
        }
    }
}
```

### 분석기에서 제공하는 옵션 정보
|파라미터|설명
|---|---
|user_words|사용자 사전을 정의한다.(기본값: [ ])
|user_idct_path|사용자 사전 파일의 경로를 정의한다. 해당 파일은 엘라스틱서치의 config 폴더 밑에 생성한다.
|decompoud|복합명사 분해 여부를 정의한다.(기본값: true)
|deinflect|활용어의 원형을 추출한다.(기본값: true)
|index_eojeol|어절을 추출한다.(기본값: true)
|index_poses|추출할 품사를 정의한다. 품사의 정의는 아래 표 참고(예: "N", "SL", "SH", "SN", "XR", "V", "M", "UNK")
|pos_tagging|품사 태깅 여부를 정의한다.(키워드에 품사가 붙어져서 나온다, 기본값: true)
|max_unk_length|Unknown 품사 태깅의 키워드로 뽑을 수 있는 최대 길이를 정의한다.(기본값: 8)

### 품사 태그
|품사 태그명|설명
|---|---
|UNK|알 수 없는 단어
|EP|선어말어미
|E|어미
|I|독립언
|J|관계언/조사
|M|수식언
|N|체언
|S|부호
|SL|외국어
|SH|한자
|SN|숫자
|V|용언
|VCP|긍정지정사
|XP|접두사
|XS|접미사
|XR|어근


### 사전 추가
- 다수의 단어를 하나로 합해서 하나의 단어처럼 사용하는 것을 복합명사라고 한다. (예: 삼성전자)

- 검색엔진에서 "삼성"을 검색하거나 "전자"를 검색했을 때도 문서가 검색되게 하고 싶다면 색인할 때 복합명사를 분리해서 역색인해야 한다.
- 반대로 복합명사를 단일명사처럼 단어가 분리되지 않게 하고 싶은 경우 은전한닢에서는 이러한 부분들을 해결할 수 있게 사용자가 등록하는 사전을 제공하는데 이러한 사전을 "사용자 사전"이라 한다.
- 사용자 사전의 위치는 인덱스를 생성한 후 setting에 사용자 사전의 경로를 지정하면 된다.
    ```
    PUT /seunjeon_with_dic_analyzer
    {
        "settings": {
            "index": {
                "analysis":{
                    "tokenizer": {
                        "seunjeon_default_tokenizer": {
                            "index_eojeol": "false",
                            "pos_tagging": "false",
                            "user_dict_path": "dic/user_dic.csv",
                            "type": "seunjeon_tokenizer"
                        }
                    },
                    "analyzer": {
                        "korean": {
                            "filter": [
                                "lowercase"
                            ],
                        "tokenizer": "seunjeon_default_tokenizer",
                        "type": "custom"
                        }
                    }
                }
            }
        }
    }
    ```
    - 사용자 사전은 엘라스틱서치 서버가 설치된 디텍터리의 config 디렉터리 안에 생성하면 된다.
- 사용자 사전은 텀(Term)과 가중치(Weight) 형태로 구성돼 있으며, 가중치의 값에 따라 그에 따른 우선순위는 달라진다. 등록할 때는 다음과 같은 형식으로 등록한다.
    ```
    삼성전자,-100
    삼성,-50
    전자,-50
    ```
  
 ## Nori 형태소 분석기
- 루씬 프로젝트에서 공식적으로 제공되는 한글 형태소 분석기로 엘라스틱서치 6.4버전에서 공식적으로 릴리즈 되었다.
- 기존 형태소 분석기에 비해 30% 이상 빠르고 메모리 사용량도 현저하게 줄었으며, 시스템 전반에 영향을 주지 않게 최적화 되었다.

### 설치방법
```
bin/elasticsearch-plugin install analysis-nori
```

### Nori 분석기의 구성
1. nori_tokenizer 토크나이저
    - 토크나이저는 형태소를 토큰 형태로 분리하는 데 사용하고 두 가지 파라미터를 지원한다.
    1) docompound_mode
        - 토크나이저가 복합병사를 처리하는 방식을 결정
        - 복합명사가 있을 경우 단어를 어떻게 쪼갤지 결정한다.
        
            |파라미터명|파라미터값|설명|예제
            |---|---|---|---
            |decompound_mode|none|복합명사로 분리하지 않는다.|월미도<br/>영종도
            | |discard|복합명사로 분리하고 원본 데이터는 삭제한다.|잠실역=>[잠실, 역]
            | |mixed|복합명사로 분리하고 원본 데이터는 유지한다.|잠실역=>[잠실, 역, 잠실]
                   
    2) user_dictionary
        - Nori 토크나이저는 내부적으로 세종 말뭉치와 mecab-ko-dic 사전을 사용한다.
        - user_dictionary를 이용해 사용자가 정의한 명사를 사전에 추가로 등록할 수 있다.
        - 엘라스틱서치 서버가 설치된 디렉터리 아래 config/userdic_ko.txt 형태로 생성해서 사용할 수 있으며 인덱스 매핑 시 분석기의 파라미터로 사전 경로를 등록하면 된다.
        - userdict_ko.txt 파일에 명사 추가하는 방법은 명사 혹은 복합명사의 구조로 설정하면 된다.
            ```
            삼성전자
            삼성전자 삼성 전자
            ```
        - decompound_mode를 "mixed"로 설정하고 userdict_ko.txt 사용자 사전을 추가한 예
            ```
            PUT nori_analyzer
            {
                "settings": {
                    "index": {
                        "analysis": {
                            "tokenizer": {
                                "nori_user_dict_tokenizer": {
                                    "type": "nori_tokenizer",
                                    "decompound_mode": "mixed",
                                    "user_dictionary": "userdict_ko.txt"
                                }
                            },
                            "analyzer": {
                                "nori_token_analyzer": {
                                    "type": "custom",
                                    "tokenizer": "nori_user_dict_tokenizer"
                                }
                            }
                        }
                    }
                }
            }
            ```
        - nori_token_analyzer 테스트
           ```
          POST nori_analyzer/_analyze
          {
            "analyzer": "nori_token_analyzer",
            "text": "잠실역"
          }
          
          # 결과
          {
            "tokens" : [
                {
                    "token": "잠실",
                    "start_offset": 0,
                    "end_offset": 2,
                    "type": "word",
                    "position": 0
                },
                {
                    "token": "역",
                    "start_offset": 3,
                    "end_offset": 5,
                    "type": "word",
                    "position": 1
                },
                {
                    "token": "잠실역",
                    "start_offset": 5,
                    "end_offset": 6,
                    "type": "word",
                    "position": 2
                }
            ]
          }
          ```              
2. nori_part_of_speech : 토큰필터



3. nori_readingform : 토큰 필터

