# 엘라스틱서치 분석기

### 텍스트 분석 개요
- 엘라스틱서치는 루씬을 기반으로 구축된 텍스트 기반 검색엔진
- 텍스트 분석을 이해하려면 루씬이 제공하는 분석기가 어떻게 동작하는지를 먼저 이해하는 것이 중요

### Analyze API를 이용한 분석
    POST _analyze
    {
        "analyzer": "standard",
        "text": "우리나라가 좋은나라, 대한민국 화이팅"
    }
    
    # 분석 결과
    {
        "tokens": [
            {
                "token": "우리나라가",
                "start_offset": 0,
                "end_offset": 5,
                "type": "<HANGUL>",
                "position": 0
            },
            {
                "token": "좋은나라",
                "start_offset": 6,
                "end_offset": 10,
                "type": "<HANGUL>",
                "position": 1
            },            
            {
                "token": "대한민국",
                "start_offset": 12,
                "end_offset": 16,
                "type": "<HANGUL>",
                "position": 2
            },
            {
                "token": "화이팅",
                "start_offset": 17,
                "end_offset": 20,
                "type": "<HANGUL>",
                "position": 3
            }
        ]
    }                                           
    
- Token은 "우리나라가", "좋은나라", "대한민국", "화이팅"으로 분리된다. Standard Analyzer를 사용했기 때문에 별도의 형태소 분석은 이루어지지 않았다.
- "우리나라"라고 입력하면 검색이 되지 않는다. "우리나라"라는 단어가 존재하지 않기 때문이다.

### 역색인 구조
- 루씬의 색인은 역색인이라는 특수한 방식으로 구조화돼 있다.
- 책의 마지막 부분에 나열된 목록과 같이 단어와 페이지가 열거돼 있어서 단어가 등장하는 페이지를 펼쳐 내용을 확인할 수 있는 방식과 비슷하다.
- 역색인 구조
    - 모든 문서가 가지는 단어의 고유 단어 목록
    - 해당 단어가 어떤 문서에 속해 있는지에 대한 정보
    - 전체 문서에 각 단어가 몇 개 들어있는지에 대한 정보
    - 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도
- 역색인 예시
    - 다음과 같은 텍스트를 가진 2개의 문서가 있다.
        ```
        문서1
        elasticsearch is cool
      
        문서2
        Elasticsearch is great
        ```
    - 문서의 역색인을 만들기 위해 각 문서를 토큰화해야 한다. 결과물은 대략 다음과 같다.
    
        |토큰|문서번호|텀의 위치(Position)|텀의 빈도(Term Frequency)
        |---|---|---|----
        |elasticsearch|문서1|1|1
        |Elasticsearch|문서2|1|1
        |is|문서1, 문서2|2, 2|2
        |cool|문서1|3|1|
        |great|문서2|3|1
        
        - 검색어가 존재하는 문서를 찾기 위해 검색어와 동일한 토큰을 찾아 해당 토큰이 존재하는 문서 번호를 찾아가면 된다.
        - Elasticsearch와 elasticsearch는 서로 다른 단어다. 같은 단어로 인지하기 위해 색인 전 텍스트 전체를 소문자로 변환한 다음 색인하는 방법이 있다.
- 색인한다는 것은 역색인 파일을 만든다는 것이다.
- 원문 자체를 변경한다는 의미는 아니며, 색인 파일에 들어갈 토큰만 변경되어 저장되고 실제 문서의 내용은 변함없이 저장된다.
- 색인할 때 특정한 규칙과 흐름에 의해 텍스트를 변경하는 과정을 분석(Analyze)이라고 하고 해당 처리는 분석기(Analyzer)라는 모듈을 조합해서 이뤄진다.

### 분석기의 구조
분석기는 기본적으로 다음과 같은 프로세스로 동작한다.
1. 문장을 특정한 규칙에 의해 수정한다.
2. 수정한 문장을 개별 토큰으로 분리한다.
3. 개별 토큰을 특정한 규칙에 의해 변경한다.

### 분석기 용어
##### CHARACTER FILTER
- 문장을 분석하기 전에 입력 텍스트에 대해 특정한 단어를 변경하거나 HTML과 같은 태그를 제거하는 역할을 하는 필터
- 해당 내용은 텍스트를 개별 토큰화하기 전의 전처리 과정이며, ReplaceAll() 함수처럼 패턴으로 텍스트를 변경하거나 사용자가 정의한필터를 적용할 수 있다.

##### TOKENIZER FILTER
- 분석기를 구성할 때 하나만 사용할 수 있으며, 텍스트를 어떻게 나눌 것인지를 정의한다.
- 한글을 분해할 때는 한글 형태소 분석기의 Tokennizer를 사용하고, 영문을 분석할 때는 영문 형태소 분석기의 Tokenizer를 사용하는 등 상황에 맞게 적절한 Tokenizer를 사용하면 된다.
   
##### TOKEN FILTER
- 토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환한다.
- 불필요한 단어를 제거하거나 동의어 사전을 만들어 단어를 추가하거나 영문 단어를 소문자로 변환하는 등의 작업을 수행할 수 있다.
- Token Filter는 여러 단계가 순차적으로 이뤄지며 순서를 어떻게 지정하느냐에 따라 검색의 질이 달라질 수 있다.

### 전체 분석 프로세스
```
[문장] -> Character Filter -> [가공된 문장] -> Tokenizer Filter -> [Terms] -> Token Filter(<->사전) > [변경된 Terms] -> Index
```

### 분석기 정의 예시
    
    # 불필요한 HTML 태그를 제거하고 모두 소문자로 변형해서 인덱스를 저장
    
    PUT movie_analyzer
    {
        "settings": {
            "index": {
                "number_of_shards": 5,
                "number_of_replicas": 1
            }
        },
        "analysis": {
            "znalyzer": {
                "custom_movie_analyzer": {      # 분석기 이름 설정
                    "type": "custom",
                    "char_filter": [            # Character Filter 정의
                        "html_stript"
                    ],
                    "tokenizer": "standard",    # Tokenizer Filter 정의
                    "filter": [                 # Token Filter 정의
                        "lowercase"
                    ]
                }
            }
        }
    }            
    
### 분석기 사용법
- 분석기를 이용한 분석
    - 엘라스틱서치에서는 형태소가 어떻게 분석되는지 확인할 수 있는 _analyzer API를 제공한다.
        ```
        # 지정한 분석기를 적용했을 때 어떻게 토큰이 분리되는지 확인할 수 있다.
      
        POST _analyze
        {
            "analyzer": "standard",
            "text": "캐리비안의 해적"
        ```
    
    
- 필드를 이용한 분석
    - 인덱스를 설정할 때 분석기를 직접 설정할 수 있다.
    - custom_movie_analyzer를 title이라는 필드에 매핑했다면 다음과 같이 필드를 직접 지정해 _analyzer API를 사용할 수 있다.
        ```
        POST movie_analyzer/_analyzer
        {
            "field": "title",
            "text": "캐리비안의 해적"
        ```
- 색인과 검색 시 분석기를 각각 설정
    - 분석기는 색인할 때 사용되는 Index Analyzer와 검색할 때 사용되는 Search Analyzer로 구분해서 구성할 수 있다.
        ```
        PUT movie_analyzer
        {
            "settings": {
                "index": {
                    "number_of_shards": 5,
                    "number_of_replicas": 1
                },
                "analysis": {
                    "analyzer": {
                        "movie_lower_test_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase"
                            ]
                        },
                        "movie_stop_test_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "english_stop"
                            ]
                        }
                    },
                    "filter": {
                        "english_stop": {
                            "type": "stop",
                            "stopwords": "_english_"
                        }
                    }
                }
            },
            "mappings": {
                "_doc": {
                    "properties": {
                        "title": {
                            "type": "text",
                            "analyzer": "movie_stop_test_analyzer",
                            "search_analyzer": "movie_lower_test_analyzer"
                        }
                    }
                }
            }
        }
        ```
        - 분석기를 매핑할 때 기본적으로 "analyzer"라는 항목을 이용해 설정하게 되는데, 이는 색인 시점과  검색 시점에 모두 동일한 분석기를 사용한다는 의미다.
        - 만약 각 시점에 서로 다른 분석기를 사용하려면 "search_analyzer" 항목을 이용해 검색 시점의 분석기를 재정의 해야 한다.
    
    - 생성된 인덱스로 다음 문서 색인
        ```
        PUT movie_analyzer/_doc/1
        {
            "title": "Harry Potter and the Chamber of Secrets"
        }
        ```
        - 색인 시점에 movie_stop_test_analyzer 분석기를 사용하도록 설정했기 때문에 불용어가 제거되어 토큰화 된다.
            ```
            [harry], [potter], [chamber], [secrets]
            ```    
    - 검색
        ```
        POST movie_analyzer/_search
        {
            "query": {
                "query_string": {
                    "default_operator": "AND",
                    "query": "Chamber of Secrets"
                }
            }
        }
        ```
        - 검색할 때는 불용어 처리가 되지 않는다. 그러므로 검색어가 다음과 같이 토큰화될 것이다.
            ```
            [chamber], [of], [secrets]
            ```

### 대표적인 분석기
- Standard Analyzer
    - 아무런 정의를 하지 않고 필드의 데이터 타입을 text 데이터 타입으로 사용한다면 기본적으로 Standard Analyzer를 사용한다.
    - 공백 혹은 특수 기호를 기준으로 토큰을 분리하고 모든 문자를 소문자로 변경하는 토큰 필터를 사용한다.
    - Standard Analyzer 구성 요소
        
        |Tokenizer|Token Filter
        |---|---
        |Standard Tokenizer|Standard Token Filter<br/> Lower Case Token Filter
    - Standard Analyzer 옵션
        
        |파라미터|설명
        |---|---
        |max_token_length|최대 토큰 길이를 초과하는 토큰이 보일 경우 해당 length 간격으로 분할한다. 기본값은 255자.
        |stopwords|사전 정의된 불용어 사전을 사용한다. 기본값은 사용하지 않는다.
        |stopwords_path|불용어가 포함된 파일을 사용할 경우의 서버의 경로로 사용한다.
    - Standard Analyzer를 이용한 문장 분석 예
        ```
        POST movie_analyzer/_analyzer
        {
          "analyzer": "standard",
          "text": "Harry Potter and the Chamber of Secrets"
        }
      
        # 분석 결과
        [harry, potter, and, the, chamber, of, secrets]
        ```
      
- Whitespace 분석기
    - 공백 문자열을 기준으로 토큰을 분리하는 간단한 분석기
    - Whitespace 분석기 구성 요소
        
        |Tokenizer|Token Filter
        |---|---
        |Whitespace Tokenizer|없음
    - Whitespace 분석기를 이용한 문장 분석 예
        ```
        POST movie_analyzer/_analyze
        {
            "analyzer": "whitespace",
            "text": "Harry Potter and the Chamber of Secrets"
        }
        
        # 분석 결과
        [Harry, Potter, and, the, Chamber, of, Secrets]
        ```
      
- Keyword 분석기
    - 전체 입력 문자열을 하나의 키워드처럼 처리(토큰화 작업 하지 않음)
    - Keyword 분석기 구성 요소
    
        |Tokenizer|Token Filter
        |---|---
        |Keyword Tokenizer|없음
    - Keyword 분석기를 이용한 문장 분석
        ```
        POST movie_analyzer/_analyzer
        {
            "analyzer": "keyword",
            "text": "Harry Potter and the Chamber of Secrets"
        }
      
        # 분석 결과
        [Harry Potter and the Chamber of Secrets]
        ```

      